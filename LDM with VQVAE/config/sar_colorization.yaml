model_params:
  image_size: 256

dataset_params:
  name: 'sar'
  im_path: 'data'  # Path to data folder containing SAR_Dataset
  im_size: 256
  im_channels: 3
  im_ext: 'png'

diffusion_params:
  num_timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02

autoencoder_params:
  z_channels: 4
  codebook_size: 8192
  down_channels: [64, 128, 256, 256]
  mid_channels: [256, 256]
  down_sample: [True, True, True]
  attn_down: [False, False, False]
  norm_channels: 32
  num_heads: 1
  num_down_layers: 2
  num_mid_layers: 2
  num_up_layers: 2

ldm_params:
  down_channels: [256, 384, 512, 768]
  mid_channels: [768, 512]
  down_sample: [True, True, True] 
  attn_down: [False, True, True]
  time_emb_dim: 512
  norm_channels: 32
  num_heads: 8
  conv_out_channels: 128
  num_down_layers: 2
  num_mid_layers: 2
  num_up_layers: 2
  condition_config:
    condition_types: ['text']
    text_condition_config:
      text_embed_model: 'clip'  # or 'bert'
      text_embed_dim: 512       # 512 for CLIP, 768 for BERT
      cond_drop_prob: 0.1

train_params:
  seed: 42
  task_name: 'sar_colorization_ldm'
  
  # VQVAE training params
  vqvae_epochs: 30
  vqvae_lr: 4.5e-6
  vqvae_batch_size: 4
  disc_start: 10000
  disc_weight: 0.5
  codebook_weight: 1
  commitment_beta: 0.25
  perceptual_weight: 1
  autoencoder_acc_steps: 1
  disc_acc_steps: 1
  
  # LDM training params  
  ldm_epochs: 100
  ldm_lr: 5.0e-5
  ldm_batch_size: 4
  autoencoder_acc_steps: 1
  cf_guidance_scale: 7.5
  
  # Checkpoints and outputs
  vqvae_autoencoder_ckpt_name: 'vqvae_autoencoder_ckpt.pth'
  vqvae_discriminator_ckpt_name: 'vqvae_discriminator_ckpt.pth' 
  ldm_ckpt_name: 'ldm_ckpt.pth'
  vqvae_latent_dir_name: 'vqvae_latents'
  
  # Inference params
  num_samples: 4
  num_grid_rows: 2
  
  # Whether to save latents during autoencoder inference for faster LDM training
  save_latents: True